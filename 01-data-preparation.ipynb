{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MovieLens Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!rm ml-100k.zip\n",
    "!rm -rf ml-100k\n",
    "!wget -O ml-100k.zip http://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
    "!unzip ml-100k.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "user_path = './ml-100k/u.user'\n",
    "item_path = './ml-100k/u.item'\n",
    "user_item = './ml-100k/u.data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observe user data \n",
    "* 913 users within this dataset \n",
    "* contains user id, age, gender occupation, zipcode with delimiter '|'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = pd.read_csv(user_path, names=['uid','age','gender','occupation','zipcode'],  sep='|')\n",
    "user_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observe item datasets \n",
    "* 1681 films \n",
    "* Each row with information - item id, title, release date, video release date, imdb url and 19 columns indicates the genres it belongs to \n",
    "* genres include - 'unknown','Action', 'Adventure', 'Animation', 'Childrens' , 'Comedy' , 'Crime','Documentary','Drama' ,'Fantasy' , 'Film-Noir' , 'Horror' , 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = ['unknown','Action' , 'Adventure', 'Animation', 'Childrens' , 'Comedy' , 'Crime', \\\n",
    "                                        'Documentary', 'Drama' ,'Fantasy' , 'Film-Noir' , 'Horror' , 'Musical', \\\n",
    "                                        'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\n",
    "\n",
    "\n",
    "item_df = pd.read_csv(item_path, names=['iid','title','release_date','video_release_date', 'imdb url'] + genres,  sep='|', encoding = \"ISO-8859-1\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observe rating dataset\n",
    "* 90640 entries \n",
    "* Each row contains item id, user id, rating and timestamp when rating is given "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ui_interaction = pd.read_csv(user_item, names=['iid', 'uid', 'rating', 'timestamp'], sep='\\t')\n",
    "ui_interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = ui_interaction.groupby('uid').timestamp.rank(pct=True, method='first')\n",
    "data = ui_interaction.join((ranks>0.9).to_frame('holdout'))\n",
    "test_data = data[data['holdout']].drop('holdout', axis=1)\n",
    "train_data = data[~data['holdout']].drop('holdout', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = ui_interaction[(ui_interaction['timestamp']<=888700934) ] #20210426\n",
    "test_data = ui_interaction[(ui_interaction['timestamp']>888700934) & (ui_interaction['timestamp']<=893286638)] #20210426 - 20210429\n",
    "rating_threshold=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propensity Score For Bias Correction "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "item_pop = ui_interaction.groupby(['iid']).sum().reset_index()[['iid', 'rating']]\n",
    "item_pop = item_pop.rename(columns={\"rating\":\"pop\"})\n",
    "pop_dict = item_pop.set_index('iid').to_dict('index')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import math\n",
    "\n",
    "def normalize(iid, rating):\n",
    "    if iid in pop_dict: \n",
    "        return rating/( math.log(pop_dict[iid]['pop'], 2) + 2) \n",
    "    else:\n",
    "        return rating\n",
    "    \n",
    "\n",
    "\n",
    "train_data['rating'] = train_data.apply(lambda x:normalize(x['iid'], x['rating']), axis=1)\n",
    "test_data['rating'] = test_data.apply(lambda x:normalize(x['iid'], x['rating']), axis=1)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_data.hist(column='rating', bins=100) "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "rating_threshold=0.35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# merge item_df and user_df for further observation \n",
    "user_item_df = train_data.merge(item_df, on=['iid'])\n",
    "user_item_df = user_item_df.merge(user_df, on=['uid'])\n",
    "user_item_df.head() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_user_item_df = test_data.merge(item_df, on=['iid'])\n",
    "test_user_item_df = test_user_item_df.merge(user_df, on=['uid'])\n",
    "test_user_item_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observe - Gender v.s. Genre "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_heat_map(df, figsize=(10,7)): \n",
    "    df = df.div(df.sum(axis=1), axis=0)     \n",
    "    plt.subplots(figsize=figsize)\n",
    "    sns.heatmap(df)\n",
    "\n",
    "gender = user_item_df[user_item_df['rating']>rating_threshold][['gender']+genres].groupby(['gender']).sum()\n",
    "plot_heat_map(gender, figsize=(10,2))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observe - Occupation v.s. Genre "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupation = user_item_df[user_item_df['rating']>rating_threshold][['occupation']+genres].groupby(['occupation']).sum()\n",
    "plot_heat_map(occupation, figsize=(10,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observe - Age v.s. Genre "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_df['age_segment']=user_item_df['age']//10\n",
    "test_user_item_df['age_segment'] = test_user_item_df['age']//10\n",
    "age = user_item_df[user_item_df['rating']>rating_threshold][['age_segment']+genres].groupby(['age_segment']).sum()\n",
    "plot_heat_map(age, figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_df['loc']=user_item_df['zipcode'].apply(lambda x: x[:3])\n",
    "test_user_item_df['loc']= test_user_item_df['zipcode'].apply(lambda x: x[:3])\n",
    "loc = user_item_df[user_item_df['rating']>rating_threshold][['loc']+genres].groupby(['loc']).sum()\n",
    "plot_heat_map(loc, figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def to_year(x):\n",
    "    try: \n",
    "        return int(datetime.strptime(x, \"%d-%b-%Y\").timetuple()[0])\n",
    "    except: \n",
    "        return None\n",
    "user_item_df['year'] = user_item_df['release_date'].apply(lambda x: to_year(x))\n",
    "test_user_item_df['year'] = test_user_item_df['release_date'].apply(lambda x: to_year(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_df[user_item_df['rating']>rating_threshold][['age_segment']+['year']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = user_item_df[user_item_df['rating']>rating_threshold][['age_segment']+['year']]\n",
    "year['count'] = 1 \n",
    "year = year.groupby(['age_segment', 'year']).sum().reset_index()\n",
    "year = year.pivot(index='age_segment', columns='year', values=['count'])\n",
    "year = year.fillna(0)\n",
    "\n",
    "year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heat_map(year, figsize=(10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observe user profile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_genre_profile = user_item_df[user_item_df['rating']>rating_threshold][['uid']+genres].groupby(['uid']).sum().reset_index()\n",
    "\n",
    "total_features = user_item_df.merge(user_genre_profile, on=['uid'])\n",
    "test_total_features = test_user_item_df.merge(user_genre_profile, on=['uid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_total_features = test_total_features.fillna(0)\n",
    "test_total_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_features = pd.get_dummies(total_features, columns=['gender', 'occupation', 'loc'])\n",
    "test_total_features = pd.get_dummies(test_total_features, columns=['gender', 'occupation', 'loc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install xgboost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "import shap\n",
    "\n",
    "total_features=(total_features - total_features.mean()) / total_features.std()\n",
    "y = total_features['rating']\n",
    "X = total_features.drop(['rating', 'iid', 'uid', 'title', 'release_date', 'imdb url', 'zipcode'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "model = xgboost.XGBRegressor().fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "test_total_features = (test_total_features - total_features.mean()) / total_features.std()\n",
    "test_y = test_total_features['rating']\n",
    "test_X = test_total_features.drop(['rating', 'iid', 'uid', 'title', 'release_date', 'imdb url', 'zipcode'], axis=1)\n",
    "\n",
    "y_pred = model.predict(test_X)\n",
    "\n",
    "\n",
    "math.sqrt(mean_squared_error(y_pred, test_y))/test_y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(model)\n",
    "shap_values = explainer(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.beeswarm(shap_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df['loc'] = user_df['zipcode'].apply(lambda x:x[0:3])\n",
    "user_demo_df = user_df[['uid', 'age', 'gender', 'occupation']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_raw = user_demo_df.merge(user_genre_profile, on=['uid'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oh_user = pd.get_dummies(user_raw, columns=['gender', 'occupation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oh_user = oh_user.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy\n",
    "import matplotlib.pyplot as plot\n",
    "import pandas \n",
    "\n",
    "df_normalized=(oh_user - oh_user.mean()) / oh_user.std()\n",
    "df_normalized = df_normalized.drop(['uid'], axis=1)\n",
    "pca = PCA(n_components=5)\n",
    "principal_components = pca.fit_transform(df_normalized)\n",
    "\n",
    "\n",
    "print(pca.explained_variance_ratio_)\n",
    "plot.plot(pca.explained_variance_ratio_)\n",
    "plot.ylabel('Explained Variance')\n",
    "plot.xlabel('Components')\n",
    "plot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "principalDf = pd.DataFrame(data = principal_components\n",
    "             , columns = ['principal component 1', 'principal component 2', 'principal component 3', 'principal component 4', 'principal component 5'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax.set_title('2 component PCA', fontsize = 20)\n",
    "ax.scatter(principalDf['principal component 1']\n",
    "               , principalDf['principal component 2']\n",
    "               , s = 50)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_principal = pd.concat([oh_user[['uid']], principalDf], axis=1)\n",
    "user_principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_principal.to_csv('user_principal.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_df.to_pickle(\"user_item_df.p\")\n",
    "item_df.to_pickle(\"item_df.p\")\n",
    "oh_user.to_pickle(\"user_df.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv('./ml-100k/train.csv', index=None)\n",
    "test_data.to_csv('./ml-100k/test.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store rating_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
